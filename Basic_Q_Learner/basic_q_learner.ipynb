{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Q Learner\n",
    "- Tutorial from deeplizar.com\n",
    "- https://deeplizard.com/learn/video/HGeI30uATws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Q-Table\n",
    "A Q-Table is matrix that maps every state with every action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total actions that we can do \n",
    "action_space_size = env.action_space.n\n",
    "# Get all the possible states\n",
    "state_space_size = env.observation_space.n\n",
    "# Create the table\n",
    "q_table = np.zeros((state_space_size,action_space_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Q-Learning Parameters\n",
    "- num_episodes = number of episodes we want the agent to play during training\n",
    "- max_steps_per_episode = we define a maximum number of steps that our agent is allowed to take within a single episode\n",
    "- learning_rate = alpha symbol from Bellman Equation\n",
    "- discount_rate = gamma symbol from Bellma Equation\n",
    "\n",
    "#### The next four parameretes are all related to exploration-exploitation\n",
    "- The max and min are just bounds to how large or small our exploration rate can be. Remember, the exploration rate was represented with the symbol E(in greek) when we discussed it previously.\n",
    "- exploration_decay_rate to 0.01 to determine the rate at which the exploration_rate will decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} q^{new}\\left( s,a\\right) =\\left( 1-\\alpha \\right) ~\\underset{\\text{old value} }{\\underbrace{q\\left( s,a\\right) }\\rule[-0.05in]{0in}{0.2in} \\rule[-0.05in]{0in}{0.2in}\\rule[-0.1in]{0in}{0.3in}}+\\alpha \\overset{\\text{ learned value}}{\\overbrace{\\left(\n",
    "                                                R_{t+1}+\\gamma \\max_{a^{^{\\prime }}}q\\left( s^{\\prime },a^{\\prime }\\right) \\right) }} \\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding The Q-Learning Algorithm Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all of the rewards we’ll get from each episode.\n",
    "# This will be so we can see how our game score changes over time.\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    # The done variable just keeps track of whether or not our episode is finished\n",
    "    done = False\n",
    "    # We need to keep track of the rewards within the current episode as well,\n",
    "    # so we set rewards_current_episode to 0 since we start out with no rewards\n",
    "    # at the beginning of each episode.\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off (Epsilon Greedy Strategy)\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])# Exploit\n",
    "        else:\n",
    "            action = env.action_space.sample()# Explore\n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        # Add new reward\n",
    "        rewards_current_episode += reward\n",
    "        if done == True: \n",
    "            break\n",
    "    # Exploration rate decay (Epsilon Greedy Strategy)\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After all episodes complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.1300000000000001\n",
      "2000 :  0.3910000000000003\n",
      "3000 :  0.5540000000000004\n",
      "4000 :  0.5540000000000004\n",
      "5000 :  0.5430000000000004\n",
      "6000 :  0.6150000000000004\n",
      "7000 :  0.6260000000000004\n",
      "8000 :  0.6920000000000005\n",
      "9000 :  0.7150000000000005\n",
      "10000 :  0.6850000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the training results\n",
    "\n",
    "Let’s take a second to understand how we can interpret these results. Our agent played 10,000 episodes. At each time step within an episode, the agent received a reward of 1 if it reached the frisbee, otherwise, it received a reward of 0. If the agent did indeed reach the frisbee, then the episode finished at that time-step.\n",
    "\n",
    "So, that means for each episode, the total reward received by the agent for the entire episode is either 1 or 0. So, for the first thousand episodes, we can interpret this score as meaning that 16% of the time, the agent received a reward of 1 and won the episode. And by the last thousand episodes from a total of 10,000, the agent was winning 70% of the time.\n",
    "\n",
    "By analyzing the grid of the game, we can see it’s a lot more likely that the agent would fall in a hole or perhaps reach the max time steps than it is to reach the frisbee, so reaching the frisbee  of the time isn’t too shabby, especially since the agent had no explicit instructions to reach the frisbee. It learned that this is the correct thing to do.\n",
    "\n",
    "- SFFF\n",
    "- FHFH\n",
    "- FFFH\n",
    "- HFFG\n",
    "\n",
    "Lastly, we print out our updated Q-table to see how that has transitioned from its initial state of all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.53582245 0.51979907 0.52721489 0.51933078]\n",
      " [0.12806382 0.26661824 0.26822145 0.50802356]\n",
      " [0.42177449 0.39301349 0.36889628 0.46498052]\n",
      " [0.27266155 0.20385012 0.31470512 0.4516567 ]\n",
      " [0.55009791 0.40111389 0.35059276 0.32504156]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.26728887 0.05444099 0.21566352 0.11153021]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40694036 0.26034254 0.46378143 0.5632143 ]\n",
      " [0.39479447 0.58970032 0.49878607 0.44215101]\n",
      " [0.48296618 0.49704292 0.36535829 0.35428938]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39875038 0.46915578 0.68270213 0.56277737]\n",
      " [0.75267538 0.80856986 0.75122684 0.76213033]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Watch the agent play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        # Choose action with highest Q-value for current state\n",
    "        action = np.argmax(q_table[state,:])  \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                # Agent reached the goal and won episode\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode \n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
